{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMFBDNxYq15XGMgkUJroDNI"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"NduZGDkBuTXG","colab_type":"text"},"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive/\")\n","%cd /content/gdrive/My\\ Drive/work_space/Chatbot/NLU/Mark1\n","%pip install import_ipynb\n","import import_ipynb"]},{"cell_type":"code","metadata":{"id":"FznsfPKFlVQI","colab_type":"code","outputId":"0e51dc8b-92ad-4a29-950a-2dcdf52137a8","executionInfo":{"status":"ok","timestamp":1589013333600,"user_tz":-270,"elapsed":52370,"user":{"displayName":"Farzad Yousefi","photoUrl":"","userId":"17223507942772645910"}},"colab":{"base_uri":"https://localhost:8080/","height":471}},"source":["%pip install persian\n","%pip install hazm\n","import hazm\n","import persian\n","import os\n","import re\n","import pandas as pd\n","import DataSet\n","import numpy as np\n","from hazm import *\n","if os.path.exists('../resources/postagger.model'):\n","  tagger = POSTagger(model='../resources/postagger.model')\n","else:\n","  raise ValueError(\"Wrong path is POSTagger directory\")\n","stemmer = Stemmer()\n","lemmatizer = Lemmatizer()\n","from nltk import ngrams\n","vocabs,IDF = 0,0"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: persian in /usr/local/lib/python3.6/dist-packages (0.4.0)\n","Collecting hazm\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/13/5a7074bc11d20dbbb46239349ac3f85f7edc148b4cf68e9b8c2f8263830c/hazm-0.7.0-py3-none-any.whl (316kB)\n","\u001b[K     |████████████████████████████████| 317kB 5.1MB/s \n","\u001b[?25hCollecting nltk==3.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/09/3b1755d528ad9156ee7243d52aa5cd2b809ef053a0f31b53d92853dd653a/nltk-3.3.0.zip (1.4MB)\n","\u001b[K     |████████████████████████████████| 1.4MB 19.9MB/s \n","\u001b[?25hCollecting libwapiti>=0.2.1; platform_system != \"Windows\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/0f/1c9b49bb49821b5856a64ea6fac8d96a619b9f291d1f06999ea98a32c89c/libwapiti-0.2.1.tar.gz (233kB)\n","\u001b[K     |████████████████████████████████| 235kB 51kB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.3->hazm) (1.12.0)\n","Building wheels for collected packages: nltk, libwapiti\n","  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nltk: filename=nltk-3.3-cp36-none-any.whl size=1394468 sha256=cb4f70573e537a288af0313d1f3e35bc94d85f31c0f65e706b5a6559a1535d04\n","  Stored in directory: /root/.cache/pip/wheels/d1/ab/40/3bceea46922767e42986aef7606a600538ca80de6062dc266c\n","  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp36-cp36m-linux_x86_64.whl size=154141 sha256=844870563b14e45d11f7e67a8c54bfa679c5e9d5fb883a84856722a32f03cf57\n","  Stored in directory: /root/.cache/pip/wheels/66/15/54/4510dce8bb958b1cdd2c47425cbd1e1eecc0480ac9bb1fb9ab\n","Successfully built nltk libwapiti\n","Installing collected packages: nltk, libwapiti, hazm\n","  Found existing installation: nltk 3.2.5\n","    Uninstalling nltk-3.2.5:\n","      Successfully uninstalled nltk-3.2.5\n","Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n","importing Jupyter notebook from DataSet.ipynb\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"y66XYwv9mdgD","colab_type":"code","colab":{}},"source":["def normalize(text):\n","  #Basic ///\n","  norm = Normalizer()\n","  text = persian.convert_ar_characters(text)\n","  text = persian.convert_ar_numbers(text)\n","  text = text.strip()\n","  text = norm.normalize(persian.convert_en_numbers(text))\n","\n","  #Advanced\n","  sent_past = []\n","  sent_future=[]\n","\n","  pos = tagger.tag(word_tokenize(text))\n","  for word,p in pos :\n","    if \"V\" != p[0]:\n","      w = stemmer.stem(word) if len(stemmer.stem(word).strip()) > 1 else \"\"\n","      sent_past.append(w)\n","      sent_future.append(w)\n","      \n","\n","    else :\n","        lem_res = lemmatizer.lemmatize(word).split(\"#\")\n","        sent_past.append(lem_res[0])\n","        try:\n","          sent_future.append(lem_res[1])\n","        except:\n","          sent_future.append(lem_res[0])\n","\n","  sent_past=  \" \".join(sent_past)\n","  sent_future= \" \".join(sent_future)\n","  res = [sent_past,sent_future] if sent_past != sent_future else [sent_past]\n","  return res\n","\n","# normalize(\"می‌خوام .\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"O4dVozWRwTfH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"a97f04c7-beec-4ae9-a3d4-6b098748a3f8","executionInfo":{"status":"ok","timestamp":1589016364688,"user_tz":-270,"elapsed":927,"user":{"displayName":"Farzad Yousefi","photoUrl":"","userId":"17223507942772645910"}}},"source":["[1,2,3,4][:2]"],"execution_count":98,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1, 2]"]},"metadata":{"tags":[]},"execution_count":98}]},{"cell_type":"code","metadata":{"id":"DnFKtj_wo1dq","colab_type":"code","colab":{}},"source":["def __to_normilize(row,**kywrd):\n","  res = normalize(row[\"text\"])\n","  if len(res) > 1:\n","    row[\"sent_future\"] = res[1]\n","    row[\"sent_past\"] = res[0]\n","  else:\n","    row[\"sent_past\"] = res[0]\n","  return row\n","\n","def __to_category(row,**kywd):\n","  categories = kywd[\"categories\"]\n","  \n","  label = categories.index(row[\"intent\"])\n","  return label\n","\n","def __docs_prepare(docs):\n","  temp = docs.apply(__to_normilize,axis=1)\n","  f = temp[[\"sent_future\",\"intent\"]]\n","  p =temp[[\"sent_past\",\"intent\"]]\n","  f.columns=[\"text\",\"intent\"]\n","  p.columns=[\"text\",\"intent\"]\n","  res = f.append(p).dropna()\n","\n","  classes = list(set(res[\"intent\"]))\n","  labels = res.apply(__to_category,categories=classes,axis=1)\n","  return res,labels,classes\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1PAFg77coegq","colab":{}},"source":["\n","def __to_ngram(row,**kywrd):\n","  intent_ngram = kywrd[\"intent_ngram\"]\n","  text =intent_ngram[intent_ngram[\"intent\"] == row[\"intent\"]][\"text\"].values[0]\n","   \n","  ntext = normalize(re.sub(\"\\((\"+row[\"name\"]+\")\\)\",\"%%%%\",text ))\n","  # if rowprint(text)\n","  ngrams_pattern = ngrams(word_tokenize(ntext[1]),3),ngrams(word_tokenize(ntext[0]),3)\n","  grams_df = []\n","\n","  for ngram in ngrams_pattern:\n","    for gram in ngram:\n","      if \"٪٪٪٪\" in gram[:2]:\n","        grams_df.append(\" \".join(gram))\n","  row[\"ngrams\"] = grams_df\n","  return row\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vafhIjGTD7pg","colab_type":"text"},"source":["docs,entities,slots,intent_ngram = DataSet.loader(lang = \"en\" ,path= \"../dataset\")\n","\n","for i,row in slots.iterrows()[1]:\n","row=slots.iloc[4]\n","text =intent_ngram[intent_ngram[\"intent\"] == row[\"intent\"]][\"text\"].values[0]\n","  \n","ntext = normalize(re.sub(\"\\((\"+row[\"name\"]+\")\\)\",\"٪٪٪٪\",text ))\n","# if rowprint(text)\n","ngrams_pattern = ngrams(word_tokenize(ntext[1]),3),ngrams(word_tokenize(ntext[0]),3)\n","grams_df = []\n","\n","for ngram in ngrams_pattern:\n","  for gram in ngram:\n","    if \"٪٪٪٪\" in gram[:1]:\n","      grams_df.append( \" \".join(gram))\n","row[\"ngrams\"] = grams_df\n","grams_df[-1]"]},{"cell_type":"code","metadata":{"id":"Yl-8wWULmBaR","colab_type":"code","colab":{}},"source":["def prepare_dataset(docs,slots,intent_ngram):\n","  docs,labels,classes= __docs_prepare(docs)\n","  slots = slots.apply(__to_ngram,intent_ngram=intent_ngram,axis=1)\n","\n","  return docs,labels,classes,slots\n","docs,entities,slots,intent_ngram = DataSet.loader(lang = \"en\" ,path= \"../dataset\")  \n","docs,labels,classes,slots = prepare_dataset(docs,slots,intent_ngram)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9sZ6ZqIWUZ4R","colab_type":"code","colab":{}},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","def docs2vectors (docs):\n","  global vocabs\n","  global IDF\n","  vocabs = \" \".join(docs).split(\" \")\n","  vocabs = list(set([wrd.strip() for wrd in vocabs]))[1:]\n","\n","  tf_idf = TfidfVectorizer(vocabulary=vocabs).fit(docs)\n","  IDF = tf_idf.idf_\n","  vocabs = tf_idf.get_feature_names()\n","  tf_idf = pd.DataFrame(tf_idf.transform(docs).toarray())\n","  \n","  return tf_idf\n","\n","# tf_idf = docs2vectors(docs=docs[\"text\"])\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J_yHRLwWh4H1","colab_type":"code","outputId":"0babd101-69f6-4b33-8b60-2c92d59f5feb","executionInfo":{"status":"ok","timestamp":1589013343170,"user_tz":-270,"elapsed":61897,"user":{"displayName":"Farzad Yousefi","photoUrl":"","userId":"17223507942772645910"}},"colab":{"base_uri":"https://localhost:8080/","height":211}},"source":["%pip install jellyfish\n","import jellyfish\n"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Collecting jellyfish\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/80/bcacc7affb47be7279d7d35225e1a932416ed051b315a7f9df20acf04cbe/jellyfish-0.7.2.tar.gz (133kB)\n","\r\u001b[K     |██▌                             | 10kB 20.3MB/s eta 0:00:01\r\u001b[K     |█████                           | 20kB 3.2MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 30kB 3.7MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 40kB 3.0MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 51kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 61kB 3.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 71kB 4.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 81kB 4.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 92kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 102kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 112kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 122kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 143kB 4.5MB/s \n","\u001b[?25hBuilding wheels for collected packages: jellyfish\n","  Building wheel for jellyfish (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for jellyfish: filename=jellyfish-0.7.2-cp36-cp36m-linux_x86_64.whl size=73006 sha256=d4058e6bfaa3d32da40b743951b386d5252327cf797770b3647048ba3d13e86c\n","  Stored in directory: /root/.cache/pip/wheels/e8/fe/99/d8fa8f2ef7b82a625b0b77a84d319b0b50693659823c4effb4\n","Successfully built jellyfish\n","Installing collected packages: jellyfish\n","Successfully installed jellyfish-0.7.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GIXassr7hkLS","colab_type":"code","colab":{}},"source":["\n","def __distanceـdf(row , **kywrd):\n","  target = kywrd[\"target\"]\n","  col_name = kywrd[\"col_name\"]\n","  param =row[col_name] , target\n","  \n","  levenshtein = 1-(jellyfish.levenshtein_distance(*param)*2)/ (len(row[col_name])+len(target))\n","  hamming = 1-(jellyfish.hamming_distance(*param)*2)/ (len(row[col_name])+len(target))\n","\n","  row[\"levenshtein\"],row[\"hamming\"]= levenshtein,hamming\n","  return row\n","\n","def similarity(a ,b):\n","  param =a , b\n","  levenshtein = 1-(jellyfish.levenshtein_distance(*param)*2)/ (len(param[0])+len(param[1]))\n","  hamming = 1-(jellyfish.hamming_distance(*param)*2)/ (len(param[0])+len(param[1]))\n","  return levenshtein,hamming\n","\n","def Dictation_correction(target):\n","  pos = tagger.tag([target])\n","  if (pos[0][1][0] ==\"V\"):\n","    if target.startswith(\"می‌\"):\n","      target = target[3:] \n","    elif target.startswith(\"می\"):\n","      target = target[2:] \n","\n","  target = target.strip()\n","  vocabs_df = pd.DataFrame(columns=[\"vocabs\"])\n","  vocabs_df[\"vocabs\"] = vocabs\n","  search_res = vocabs_df.apply(__distanceـdf,target= target,col_name=\"vocabs\",axis=1)\n","  arg = search_res.iloc[np.argmax(search_res[\"levenshtein\"])]\n","  \n","  if arg[\"hamming\"] > 0.65 and arg[\"levenshtein\"] > 0.65  :\n","    # print(arg)\n","    return arg[\"vocabs\"] \n","  else:\n","    \n","    return 0\n","\n","# Dictation_correction(target=\"خواسم\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Swl1QX6eoKDp","colab_type":"code","colab":{}},"source":["from sklearn.feature_extraction.text import CountVectorizer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K2xHW0uFnjuR","colab_type":"code","colab":{}},"source":["def query2vector(query):\n","  global IDF\n","  global vocabs\n","  \n","  n_text = []\n","  for word in word_tokenize(Normalizer().normalize(query)):\n","    if len(word) > 1:\n","      if tagger.tag([word])[0][1][0] != \"V\": \n","        st = Dictation_correction(word)\n","        if st != 0:\n","          n_text.append(st)\n","        else:\n","          continue\n","      else:\n","        try:\n","          n_text.append(lemmatizer.lemmatize(word).split(\"#\")[np.random.randint(0,2)])\n","        except:\n","          n_text.append(lemmatizer.lemmatize(\"میخام\"))\n","\n","  n_text = \" \".join(n_text)\n","  # print(n_text)\n","  tf = CountVectorizer(vocabulary=vocabs).fit_transform(pd.Series({\"query\":n_text}))\n","  Qtfidf = tf.toarray()*IDF\n","\n","  return Qtfidf\n","\n","# query = \" میخوام درس هوش محاسباتی را اخذ کنم و بینایی را حذف کنم .\"\n","# query2vector(query)"],"execution_count":0,"outputs":[]}]}