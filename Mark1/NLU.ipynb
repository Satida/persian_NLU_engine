{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLU.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOMk60eF5Y7+ug97lDntxtr"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"2USlKvHOSR3A","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VH9UdRqq8hfH","colab_type":"text"},"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive/\")\n","%cd /content/gdrive/My\\ Drive/work_space/Chatbot/NLU/Mark1\n","%pip install import_ipynb\n","import import_ipynb"]},{"cell_type":"code","metadata":{"id":"r9Vf_r-_8t9w","colab_type":"code","outputId":"de08df57-65f3-448f-dfb7-a0c4fdbe0db6","executionInfo":{"status":"ok","timestamp":1589026676828,"user_tz":-270,"elapsed":10327,"user":{"displayName":"Farzad Yousefi","photoUrl":"","userId":"17223507942772645910"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["import re\n","import numpy as np\n","import pandas as pd\n","%pip install persian\n","%pip install hazm\n","import hazm\n","import persian\n","import os\n","import re\n","import pandas as pd\n","import DataSet\n","import numpy as np\n","from hazm import *\n","from nltk import ngrams\n","import DataSet\n","import NLP"],"execution_count":108,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: persian in /usr/local/lib/python3.6/dist-packages (0.4.0)\n","Requirement already satisfied: hazm in /usr/local/lib/python3.6/dist-packages (0.7.0)\n","Requirement already satisfied: libwapiti>=0.2.1; platform_system != \"Windows\" in /usr/local/lib/python3.6/dist-packages (from hazm) (0.2.1)\n","Requirement already satisfied: nltk==3.3 in /usr/local/lib/python3.6/dist-packages (from hazm) (3.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from libwapiti>=0.2.1; platform_system != \"Windows\"->hazm) (1.12.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DKZu6pt29VWe","colab_type":"code","colab":{}},"source":["def Start(lang,path):\n","  docs, entities ,slots,intent_ngram = DataSet.loader(lang=lang,path=path)\n","  docs,labels,classes,slots = NLP.prepare_dataset(docs,slots,intent_ngram)\n","  vectors = NLP.docs2vectors(docs=docs[\"text\"])\n","  return dict(zip([\"vectors\",\"labels\",\"classes\",\"slots\",\"entities\"],[vectors,labels,classes,slots,entities]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ax11n1KODbXa","colab_type":"code","colab":{}},"source":["from nltk import ngrams\n","\n","def find_match_entity(row ,**kywrd):\n","  #print(row[\"value\"],len(word_tokenize(Normalizer().normalize(row[\"value\"]))))\n","  words = word_tokenize(Normalizer().normalize(kywrd[\"text\"]))\n","  ng_words = ngrams(words,len(word_tokenize(Normalizer().normalize(row[\"value\"]))))\n","  sim= []\n","  for w in ng_words:\n","    sim.append(NLP.similarity(a=row[\"value\"],b=\" \".join(w))[0])\n","  res = (row[\"entity\"],row[\"value\"]) if np.max(sim) > 0.7 else 0\n","  \n","  return res"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZayY4icbCpwW","colab_type":"code","colab":{}},"source":["def __entities_extarctor(**kywrd):\n","  intent = kywrd['intent']\n","  text = kywrd[\"text\"]\n","  slots =  kywrd[\"slots\"]\n","  entities = kywrd[\"entities\"]\n","  entities =entities.set_index(\"name\")\n","  \n","  req_ent= list(set([x for sublist in slots[slots[\"intent\"] == intent][\"entities\"] for x in sublist]))\n","  entity_keys= []\n","  entity_patterns= []\n","  for e in req_ent:\n","    if entities.loc[e][\"type\"] == \"list\":\n","      for ele in entities.loc[e][\"occurrences\"]:\n","\n","        entity_keys.append({\"entity\":e,\"value\":ele[\"name\"] })\n","\n","        if len(ele[\"synonyms\"]) != 0:\n","          entity_keys += [{\"entity\":e,\"value\":x } for x in ele[\"synonyms\"]]\n","    else:\n","      entity_patterns.append((e,entities.loc[e][\"pattern\"]))\n","  \n","  ent_patt_mch = []\n","  for n,p in entity_patterns:\n","    res = re.findall(p,text)\n","    ent_patt_mch += list(zip([n]*len(res),res))\n","\n","  entity_keys = pd.DataFrame(entity_keys)\n","\n","  temp =entity_keys.apply(find_match_entity,axis=1,text = text)\n","  ents = list(temp[temp != 0]) + ent_patt_mch\n","\n","  return ents"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GX2_mqcwfEDd","colab_type":"code","colab":{}},"source":["from sklearn.ensemble import RandomForestClassifier \n","from sklearn.metrics import classification_report\n","path = \"./models\"\n","\n","\n","def Train(save_model = False,load_model = True , **kywrd):\n","  trainX,trainy = kywrd[\"vectors\"],kywrd[\"labels\"]\n","  if load_model:\n","    clf = __load_classifier(path)\n","  else:\n","    clf = RandomForestClassifier(max_depth=12,max_features=50,n_estimators=20)\n","    clf.fit(trainX,trainy)\n","  if save_model:\n","    __save_model(clf,path)\n","  print(\"train Score :\", clf.score(trainX,trainy))\n","  kywrd[\"classifier\"] =clf\n","  return kywrd\n","\n","#classifier = engine_train(vectors = tf_idf , labels= labels)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YtgvyCy7WZRg","colab_type":"code","colab":{}},"source":["import pickle\n","import datetime\n","def __save_model(classifier,path):\n","  try:\n","    cls_name =str(type(classifier))\n","    cls_name = cls_name[cls_name.rfind(\".\")+1:-2]\n","    with open(os.path.join(path,'{0}{1}.cpkl'.format(cls_name, str(datetime.datetime.now())[5:16])), 'wb') as f:\n","      pickle.dump(classifier, f)\n","    return True\n","  except err:\n","    raise ValueError(err)\n","\n","import glob\n","def __load_classifier(path):\n","  \n","  for i,sp in enumerate(glob.glob(os.path.join(path,\"*.cpkl\"))):\n","    print(i,\")\",sp)\n","  select = input(\"select model that you want load or 'exit':\")\n","  if select == \"exit\":\n","    return 0\n","  try:\n","    path = glob.glob(os.path.join(path,\"*.cpkl\"))[int(select)]\n","    with open(path, 'rb') as f:\n","      classifier = pickle.load(f)\n","    return classifier\n","  except err:\n","    raise ValueError(err)\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fqorSmg7a6Ha","colab_type":"code","colab":{}},"source":["# intent,ents,slots = Predict(\"میخوام درس هوش محاسباتی و اخلاق اسلامی را اخذ کنم 95125856 و هوش مصنوعی را حذف کنم .\",**ready_to_predict)\n","# text =\"میخوام درس هوش محاسباتی و اخلاق اسلامی را اخذ کنم 95125856 و هوش مصنوعی را حذف کنم .\"\n","\n","\n","def __to_ngram(row,**kywrd):\n","  text = kywrd[\"text\"]\n","  slots = kywrd[\"slots\"]\n","  \n","  if row[\"val\"] in text:\n","    text = text.replace(row[\"val\"],\"٪٪٪٪\")\n","  else:\n","    for wrong in ngrams(word_tokenize(text),len(word_tokenize(row[\"val\"]))):\n","      if NLP.similarity(row[\"val\"],\" \".join(wrong))[0] > 0.65:\n","        text = text.replace(\" \".join(wrong),\"٪٪٪٪\")\n","        break\n","      \n","\n","  for word in row[\"another_keys\"]:\n","    text = text.replace(word, \"\")\n","  \n","  grams = ngrams(word_tokenize(NLP.normalize(text)[np.random.randint(0,2)]),3)\n","  text_grams = []\n","  for g in grams:\n","    if \"٪٪٪٪\" in g[:2]:\n","      text_grams.append(\" \".join(g))\n","\n","  res = []\n","  for tg in text_grams:\n","    for sg in list(slots[slots[\"id\"] == row[\"slot\"] ][\"ngrams\"])[0]:\n","      res.append(NLP.similarity(tg,sg))\n","\n","  row[\"similarity\"] = np.average(res)\n","  row[\"name\"] = slots[slots[\"id\"] == row[\"slot\"]][\"name\"].iloc[0]\n","  return row\n","\n","\n","def __slots_extractor(ents,slots,text,intent):\n","\n","  slt = slots[slots[\"intent\"] == intent]\n","  slots_per_entity = pd.DataFrame(columns=[\"val\",\"another_keys\",\"slot\"])\n","  for n_ent , v_ent in ents:\n","    for i,slot in slt.iterrows():\n","      if n_ent in slot[\"entities\"] and slot[\"intent\"] == intent:\n","        slots_per_entity = slots_per_entity.append(pd.Series({\"val\":v_ent,\"another_keys\":[v for n , v in ents if v !=v_ent ] ,\"slot\":slot[\"id\"]}),ignore_index=True)\n","\n","  found_slot_df = slots_per_entity.apply(__to_ngram,text=text,slots=slots,axis=1)\n","  result = []\n","  for val,sim in found_slot_df.groupby(\"val\"):\n","    idx = np.argmax(sim[\"similarity\"])\n","    result.append((val,sim[\"name\"].iloc[idx]))\n","  \n","  return result\n","\n","\n","#slots[slots[\"id\"] == \"gkv0BbM1njQKhwmsudG5G\" ][\"name\"][0]\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HnkeOd07AJef","colab_type":"code","colab":{}},"source":["def Predict(text , **kywrd):\n","  vec = NLP.query2vector(text)\n","  intent =kywrd[\"classes\"][int(kywrd[\"classifier\"].predict(vec))]\n","  ents = __entities_extarctor(intent= intent , text = text ,**kywrd)\n","  found_slots = __slots_extractor(ents,kywrd[\"slots\"],text,intent)\n","  return intent,ents,found_slots\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iTXL3MpMd-HZ","colab_type":"code","colab":{}},"source":["def ShowResult(result):\n","  intent,ents,found_slots =  result\n","  print()\n","  print(\"Intent : \",\"=\"*40)\n","  print(\"\\t\",intent)\n","\n","  print(\"\\nEntities : \\n\",\"=\"*40)\n","  [print(\"\\t\",x,\" : \",y) for y,x in ents]\n","  print(\"\\nSlots    : \\n\",\"=\"*40)\n","  [print(\"\\t\",x,\" : \",y) for y,x in found_slots]"],"execution_count":0,"outputs":[]}]}