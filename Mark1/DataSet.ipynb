{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DataSet.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNZHIynfhelYrKgw4sW19+E"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"-wqO-2Gzyh0x","colab_type":"code","outputId":"1feb77ac-6631-4b16-8cf7-d910d915571e","executionInfo":{"status":"ok","timestamp":1588954116288,"user_tz":-270,"elapsed":1136,"user":{"displayName":"Farzad Yousefi","photoUrl":"","userId":"17223507942772645910"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["# from google.colab import drive\n","# drive.mount(\"/content/gdrive/\")\n","# %cd /content/gdrive/My\\ Drive/work_space/Chatbot/NLU/Mark1\n","lang = \"en\"\n","path = \"../dataset\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n","/content/gdrive/My Drive/work_space/Chatbot/NLU/Mark1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-C8jv1KP-7r2","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import glob \n","import os \n","import re\n","import json\n","import numpy as np\n","lang = 0\n","path = 0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aLKXeH04561E","colab_type":"code","colab":{}},"source":["def __docs_loader(intent):\n","  docs = pd.DataFrame(columns=[\"text\" ,\"intent\"])\n","  for uttr in intent[\"utterances\"][lang]:\n","    text = re.sub(\"\\[[^\\]]*\\]|\\((.*?)\\)\",\"\", uttr)\n","    docs = docs.append(pd.Series({\"text\":text ,\"intent\":intent[\"name\"]}),ignore_index=True)\n","  return docs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nA6jxsOS7aNg","colab_type":"code","colab":{}},"source":["def __ngram_loader(intent):\n","  intent_ngram = pd.DataFrame(columns=[\"text\",\"intent\"])\n","  text = \" \".join(intent[\"utterances\"][lang])\n","  text = re.sub(\"\\[[^\\]]*\\]\",\"\", text)\n","  intent_ngram = intent_ngram.append(pd.Series({\"text\":text ,\"intent\":intent[\"name\"]}),ignore_index=True) if len(intent[\"slots\"])!= 0  else intent_ngram\n","  return intent_ngram"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L0AMC6tT89Tl","colab_type":"code","colab":{}},"source":["def __slots_loader(intent):\n","  slots = pd.DataFrame(columns=[\"id\",\"name\",\"entities\" ,\"intent\",\"ngrams\"])\n","  for sl in intent[\"slots\"]:\n","    serie = {}\n","    serie[\"intent\"] = intent[\"name\"]\n","    for col in slots.columns[:-2]:\n","      serie[col] = sl[col]\n","    slots = slots.append(pd.Series(serie),ignore_index=True)\n","  return slots"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ssqbFxj59iHu","colab_type":"code","colab":{}},"source":["def __entities_loader(entity):\n","  entities = pd.DataFrame(columns=[\"name\",\"type\",\"pattern\",\"examples\",\"occurrences\"])\n","  serie = {}\n","  for col in entities.columns:\n","    serie[col] = entity[col]\n","  entities = entities.append(pd.Series(serie),ignore_index=True)\n","  return entities\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ftJ_gORt1idh","colab_type":"code","colab":{}},"source":["def loader(**kywrd):\n","  global lang\n","  global path\n","\n","  lang = kywrd[\"lang\"]\n","  path = kywrd[\"path\"]\n","\n","  for intent_path in glob.glob(os.path.join(path,\"intents\",\"*.json\")):\n","    with open(intent_path ,\"r\") as jf:\n","      intent = json.load(jf)\n","    \n","    try:\n","      docs=pd.concat((docs , __docs_loader(intent)),axis=0)\n","      intent_ngram=pd.concat((intent_ngram,__ngram_loader(intent)),axis=0)\n","      slots=pd.concat((slots, __slots_loader(intent)),axis=0)\n","    except:\n","      docs=__docs_loader(intent)\n","      intent_ngram=__ngram_loader(intent)\n","      slots= __slots_loader(intent)\n","    \n","    \n","  for entity_path in glob.glob(os.path.join(path,\"entities\",\"*.json\")):\n","    with open(entity_path ,\"r\") as jf:\n","      entity = json.load(jf)\n","    try:\n","      entities=pd.concat((entities , __entities_loader(entity)),axis=0)\n","    except:\n","      entities= __entities_loader(entity)\n","  \n","  return docs, entities ,slots,intent_ngram\n","\n","#docs, entities ,slots ,intent_ngram= loader(lang = \"en\" ,path= \"../dataset\")"],"execution_count":0,"outputs":[]}]}